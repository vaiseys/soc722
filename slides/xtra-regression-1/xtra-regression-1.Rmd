---
title: "More on the LRM"
author: "<small><br>Stephen Vaisey<br>Duke University</small>"
date: "<span style = 'font-size: 65%;'>Last update: `r Sys.Date()`<br><br>`r icon::fa_link()` <a href='stephenvaisey.com'><font color='F3F2F1'>stephenvaisey.com</font></a><br>`r icon::fa_twitter()` <a href='http://twitter.com/vaiseys'><font color='F3F2F1'>@vaiseys</font></a><br>`r icon::fa_github()` <a href='https://github.com/vaiseys'><font color='F3F2F1'>vaiseys</font></a></span> "
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    self_contained: false
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["left", "middle", "inverse"]
---

```{r setup, echo=FALSE, message = FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  fig.align = "center",
  fig.asp = 0.618,
  fig.retina = 3,
  fig.width = 6,
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  out.width = "80%"
)
options(knitr.table.format = "html")
options(knitr.kable.NA = '   ')

library(here)
library(knitr)
library(tidyverse)
library(broom)
theme_set(theme_minimal(base_family = "Lato"))
windowsFonts("Lato" = windowsFont("Lato")) # only needed for interactive use

```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
mono_light(
  base_color = "#00539B" ,
  white_color = "#FFFFFF" , # was F3F2F1
  black_color = "#262626" ,
  header_h1_font_size = "110px",
  header_h2_font_size = "60px",
  header_h3_font_size = "45px",
  text_font_size = "28px" ,
  code_font_size = ".65em" ,
  header_color = "#C84E00" ,
  text_slide_number_font_size = "0.6em" ,
  code_highlight_color = "rgba(161,183,13,0.5)" ,
  link_color = "#993399" ,
  text_font_google = google_font("Lato", "400", "400i", "700") ,
  code_font_google = google_font("Roboto Mono", "400") ,
  extra_css = list(
    ".remark-slide-content" = list("padding-top" = "60px") ,
    "h1" = list("margin" = "0 0 20px 0") ,
    "h2" = list("margin" = "0 0 20px 0") ,
    "h3" = list("margin" = "0 0 20px 0") ,
    "th, td" = list( "padding" = "10px" ),
    ".small" = list("font-size" = "60%"),
    ".smallish" = list("font-size" = "80%"),
    ".small-code" = list("font-size" = ".6em") ,
    ".smaller-code" = list("font-size" = ".5em") ,
    ".red" = list("color" = "red") ,
    ".blue" = list("color" = "blue") ,
    ".col-text" = list("font-size" = "80%")) # synonym for "smallish"
)

```

class: center, middle

# Ask two questions

---

class: center, middle

# Assessment time

---

## Agenda for rest of the course

1. Estimators as LRMs

2. Interactions

3. Control attempts

4. Matrix representation of regression

5. Diagnosing problems


---

class: center, middle, inverse

# Estimators as LRMs

---

## The Basque terrorism case

```{r}
d <- read_csv(here("data", "basque.csv"))
d <- d %>% 
  mutate(basque = if_else(region == "Basque Country", 1L, 0L),
         posttreat = if_else(year >= 1973, 1L, 0L ))
```

```{r basque_trend}

gd <- d %>% 
  group_by(basque, year) %>% 
  summarize( gdp = mean(gdpcap) ,
             posttreat = mean(posttreat))

ggplot(gd, aes(y = gdp, x = year, 
               group = factor(basque), 
               color = factor(basque))) +
  geom_line() +
  geom_vline(xintercept = 1972.5, linetype = 2) +
  theme(legend.position = "none")

```

---

## Three estimators as LRMs

1. Difference in means

2. Before and after

3. Difference in difference

---

## Difference in means

```{r, echo = TRUE}
diff_means <- 
  lm( gdp ~ basque , data = filter(gd, year>=1973) )
tidy(diff_means) %>% select(term, estimate, std.error)

```

---

## Before and after

```{r, echo = TRUE}
b_and_a <- 
  lm( gdp ~ posttreat , data = filter(gd, basque==1) )
tidy(b_and_a) %>% select(term, estimate, std.error)

```

---

## Difference-in-differences

```{r, echo = TRUE}
did <- 
  lm( gdp ~ posttreat + basque + posttreat:basque,
      data = gd )
tidy(did) %>% select(term, estimate, std.error)

```

---

## In equation form

**Comparing Basque to non-Basque after 1973**

$gdp_{it} = \beta_0 + \beta_1 basque_{i} + \epsilon_{it}$

--

**Comparing Basque before and after 1973**

$gdp_{it} = \beta_0 + \beta_1 after_t + \epsilon_{it}$

--

**Counterfactual D-in-D**

$gdp_{it} = \beta_0 + \beta_1 basque_i + \beta_2 after_t + \beta_3 (basque_i)(after_t) + \epsilon_{it}$

---

## Controlling for time

```{r, echo = TRUE}
gd <- gd %>% mutate(year0 = year-1973) # year-1973 (year 0 is start)
did2 <- 
  lm( gdp ~ posttreat + basque + year0 + posttreat:basque ,
      data = gd )
tidy(did2) %>% select(term, estimate, std.error)

```

--

We will talk more about "controls" soon. For now, what is a reasonable (say, 
95% confidence) estimate of how much terrorism affected the GDP of the Basque 
country?
--
 **[€162-804]**


???

The DinD estimate doesn't change. But the SE is much lower. Why? Because the 
original estimate (`did`) treated the change over time as *error*. It assumed
that year-to-year changes (other than the treatment shock) were noise.

---

class:inverse, middle, center

# Interactions

---

## What is an interaction?

An **interaction** is when one variable **moderates** the effect of another
variable.

--

$gdp_{it} = \beta_0 + \beta_1 basque_i + \beta_2 after_t + \beta_3 (basque_i)(after_t) + \epsilon_{it}$


Here the effect of being in the Basque country (vs. elsewhere) is *moderated* by
whether it's before or after 1973.

--

Equivalently, the effect of being before or after 1973 is *moderated* by 
whether a region is in the Basque country or not.

---

## Types of interactions

All interactions are fundamentally the same. But intepretation can sometimes
be challenging.

--

1. Binary $\times$ binary (e.g., diff-in-diff)

--

2. Binary $\times$ numeric

--

3. Numeric $\times$ numeric (don't worry for now)

---

## Example of binary-numeric interaction

--

```{r}
d <- haven::read_dta(here("data", "GSS2018.dta")) 
cd <- d %>% 
  select(realinc, age, degree) %>%
  filter(age <= 50) %>% 
  drop_na() %>% 
  mutate(college = if_else(degree >= 3, 1L, 0L),
         colfac = if_else(degree >= 3, "College degree", "No degree"))

ggplot(cd, aes(x = age, y = realinc, 
               color = colfac)) +
  geom_smooth(method = "lm" ) +
  labs(title = "Income by Age and Degree Status",
       subtitle = "2018 General Social Survey, ages 18-50",
       y = "Household income") +
  theme(legend.title = element_blank())
```

---

## In regression form

```{r, echo = TRUE}
lm(realinc ~ college + age + college:age, data = cd) %>% 
  tidy() %>% select(term, estimate, std.error)

```

--

$income_i = 12332 - 13591(college_i) + 384(age_i) + 980(college_i)(age_i) + \epsilon_i$ 

---

## "Centering" and interactions

- As we just saw, if the numeric variable doesn't have a meaningful zero point, 
it can make interpretation a bit complicated.

- Here $-13591$ is what the model thinks the difference should be between a college-educated zero year-old and a zero year-old without a college degree.

- This has *no effect* on the $\beta$s for $(college)$ or $(college)(age)$,
however, so if that's the only goal, it's not a problem.

--

- If necessary, this issue can be addressed in a couple of basic ways:
  + use a "demeaned" version of the numeric variable
  + use a "de-minned" version of the numeric variable
  
---

## Two equivalent approaches

.pull-left[
.col-text[
```{r, echo = TRUE}
cd <- cd %>% 
  mutate(dage = age - mean(age))
# NOTE: x*z is short for x + z + x:z
lm(realinc ~ college * dage, data = cd) %>%
  tidy() %>% select(term, estimate, std.error)

```

Now the $\beta$ on $college$ is the college/non-college difference at the *mean*
age (about 34.9 in this sample of 18-50 year-olds).

]
]

--

.pull-right[
.col-text[

```{r, echo = TRUE}
cd <- cd %>% 
  mutate(age0 = age - 18)
# NOTE: x*z is short for x + z + x:z
lm(realinc ~ college * age0, data = cd) %>%
  tidy() %>% select(term, estimate, std.error)

```

Now the $\beta$ on $college$ is the college/non-college difference at the 
*minimum* age (18 years).

]
]

---

## Other uses of interactions/moderators

- Do two groups change at different rates?

- Do supportive social networks buffer the effect of stressors?

- Do economic resources moderate the effects of divorce on children?

--

Can you think of other examples?

---

class: inverse,middle,center

# Control attempts

---

## What is "controlling"?

.center[

![](http://nickchk.com/anim/Animation%20of%20Control.gif)
]

```{r}
# from nickchk.com

```


---

## Looking closer at the simulation

```{r, echo = TRUE}
set.seed(123)
df <- data.frame(W = as.integer((1:200>100))) %>%
  mutate(X = .5+2*W + rnorm(200)) %>%
  mutate(Y = -.5*X + 4*W + 1 + rnorm(200))
glimpse(df)

```

---

## The logic of controlling

```{r, echo = TRUE}
ryw <- lm(Y ~ W, data = df)
rxw <- lm(X ~ W, data = df)

df <- df %>%
  mutate(yhat = Y - predict(ryw),  # the Y not predicted by W
         xhat = X - predict(rxw) ) # the X not predicted by W

glimpse(df)

```

---

## What's the right answer?

```{r, echo = TRUE}

lm(Y ~ X + W, data = df) %>% 
  tidy() %>% select(term, estimate, std.error) 

```

---

## Getting it manually

```{r, echo = TRUE}
lm(yhat ~ xhat, data = df) %>% 
  tidy() %>% select(term, estimate, std.error)

```

Note that the standard error won't be right because the
estimator doesn't know about the uncertainty coming from the regressions we
used to remove the influence of $W$.

---

class:middle,center, inverse

# Questions?

---

class: middle, center, inverse

# Matrix representation of regression

---

## Disclaimer

- I'm going to skip *a lot* here and go very fast.

- The idea is for you to get the gist of what is going on in estimating OLS.

- If you want to know more, you'll have to dig deeper on your own.

---

## Start simple

- Forget regression, forget matrices, open your mind!
--

- Imagine Sarah wants to buy 4 movie tickets at $9 each. How much does she 
spend?
--

$$ s = n \times p$$
--

- Now instead imagine Sarah spent $72 on 6 movie tickets. How much did each
ticket cost?
--

$$72 = 6 \times p$$
--

$$\left( \frac{1}{6} \right) 72 =  \left( \frac{1}{6} \right) 6 \times p$$
--

$$12 = p$$

---

## Doing more with matrices

- Let's say we now have 5 groups of people buying tickets and there are two
types of tickets: adult and child. We can use **matrices** to organize our work.
How much does each group spend?
--

.center[
$\mathbf{N} = \begin{bmatrix} 1 & 2 \\ 2 & 4 \\ 2 & 1 \\ 8 & 8 \\ 2 & 5 \\ \end{bmatrix} \text{  } \mathbf{p} = \begin{bmatrix} 9.00 \\ 5.00 \end{bmatrix}$
.smallish[(Adult tickets in first column, child tickets in second.)
]
]

---

## Matrix multiplication

$$\mathbf{s} = \begin{bmatrix} 19 \\ 38 \\ 23 \\ 112 \\ 43 \\ \end{bmatrix} =\begin{bmatrix} 1 & 2 \\ 2 & 4 \\ 2 & 1 \\ 8 & 8 \\ 2 & 5 \\ \end{bmatrix} \times \begin{bmatrix} 9.00 \\ 5.00 \end{bmatrix}$$

--

Two matrices must be *conformable for multiplication* to be multiplied. 
The number of ROWS on the RIGHT must be equal to the number of columns on the 
left.

---

## What if we want the prices instead?

- I've now changed the prices. Imagine we know the following:

$$\mathbf{s} = \begin{bmatrix} 27 \\ 54 \\ 30 \\ 152 \\ 62 \\ \end{bmatrix} =\begin{bmatrix} 1 & 2 \\ 2 & 4 \\ 2 & 1 \\ 8 & 8 \\ 2 & 5 \\ \end{bmatrix} \times \begin{bmatrix} ??? \\ ??? \end{bmatrix}$$
--

- How would we get the prices? We can start with the general equation here:

$$\mathbf{s} = \mathbf{N}\mathbf{p}$$
--

- If this were *scalar math* (i.e., normal math), we'd just divide both 
sides by $\mathbf{N}$ (or equivalently, multiply both sides by the **inverse**
of $\mathbf{N}$).

---

## Basic things to know before proceeding

- We need to get rid of $\mathbf{N}$ from the right side so we can get 
$\mathbf{p}$ by itself.

--

- Sadly, there's no such thing as matrix division.

--

- However, we can cancel a matrix out if we multiply it by its **inverse**.

--

- Sadly, only square matrices (same number of rows and columns) have inverses.

---

## The next step: getting square

- If we want to get rid of $\mathbf{N}$, we first have to make it square.
--

- A matrix multiplied by its transpose is always square! We are going to 
**premultiply** $\mathbf{N}$ by its transpose $\mathbf{N^T}$.
--

- I will now pass on to you the "transpose hand motion" I learned from
François Nielsen.
--

$$\mathbf{N^TN} = \begin{bmatrix} 1 & 2 & 2 & 8 & 2 \\ 2 & 4 & 1 & 8 & 5 \end{bmatrix} \times \begin{bmatrix} 1 & 2 \\ 2 & 4 \\ 2 & 1 \\ 8 & 8 \\ 2 & 5 \\ \end{bmatrix} = \begin{bmatrix} 77 & 86 \\ 86 & 110 \end{bmatrix}$$

---

## What is a matrix inverse?

- In "normal math," when you multiply a number by its inverse, you get 1.
--

- In matrix math, when you multiply a matrix by its inverse, you get the
**identity matrix** - a matrix with 1s on the diagonal and 0s everwhere else.
--

$$\begin{bmatrix} ??? & ??? \\ ??? & ??? \end{bmatrix} \times \begin{bmatrix} 77 & 86 \\ 86 & 110 \end{bmatrix}  = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$$
--

- The notation for the inverse is $\mathbf{(N^TN)^{-1}}$.

---

## How do we find the inverse?

- The short answer is: this is why we have computers!
--

- The long answer depends on the **dimension** of the matrix to invert. For
$2 \times 2$ matrices, the inverse is:

$$\begin{bmatrix} a & b  \\ c & d \end{bmatrix}^{-1} = \frac{1}{ad-bc}\begin{bmatrix} d & -b  \\ -c & a \end{bmatrix}$$ 
--

- Therefore, for our problem, we have:

$$\begin{bmatrix} 77 & 86 \\ 86 & 110 \end{bmatrix}^{-1} = \frac{1}{1074} \begin{bmatrix} 110 & -86  \\ -86 & 77 \end{bmatrix} \approx \begin{bmatrix} .102 & -.080 \\ -.080 & .072 \end{bmatrix}$$ 

---

## Putting the pieces together

Start with the basic formula:

$$\mathbf{s} = \mathbf{N}\mathbf{p}$$
--

Make $\mathbf{N}$ square on the RHS using $\mathbf{N^T}$:

$$\mathbf{N^T}\mathbf{s} = \mathbf{N^T}\mathbf{N}\mathbf{p}$$
--

Multiply both sides by the inverse of $\mathbf{N^T}\mathbf{N}$:

$$(\mathbf{N^T}\mathbf{N})\mathbf{^{-1}}\mathbf{N^T}\mathbf{s} = (\mathbf{N^T}\mathbf{N})\mathbf{^{-1}}\mathbf{N^T}\mathbf{N}\mathbf{p}$$
--

Cancel on the RHS and you're done! You've got $\mathbf{p}$ alone.

$$(\mathbf{N^T}\mathbf{N})\mathbf{^{-1}}\mathbf{N^T}\mathbf{s} = \mathbf{p}$$

---

## Solving in R

```{r}
p <- matrix(c(11.00, 
              8.00 ),
            nrow = 2)

N <- matrix( c(1, 2,
               2, 4,
               2, 1,
               8, 8,
               2, 5) ,
             nrow = 5, byrow = TRUE )

s <- N %*% p

```

.pull-left[

```{r, echo = TRUE}
N

s

```

]

--

.pull-right[

```{r, echo = TRUE}
# solve is the inverse function
# %*% means matrix multiplication

p <- solve(t(N) %*% N) %*% t(N) %*% s
p

```

So the adult tickets cost 11 dollars and the child tickets cost 8 dollars.

]

---

## Extending this to regression

Linear regression is fundamentally the same. We want to know what "price" 
(i.e., what $\beta$ weight) to put on each variable.
--

$$\mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}$$
.center[or, in slightly different notation,]

$$\mathbf{y} = \mathbf{X}\mathbf{b} + \mathbf{e}$$

---

## The blessings of (assuming) exogeneity

- If we assume *exogeneity*, we can simply declare that $\mathbf{e}$ has an
expectation of 0 and is uncorrelated with $\mathbf{X}$, meaning we can ignore it
in our calculations of $\mathbf{b}$.
--

- Therefore we have the same operation we had before to get $\mathbf{b}$ all
by itself, leaving us with:
--

$$(\mathbf{X^T}\mathbf{X})\mathbf{^{-1}}\mathbf{X^T}\mathbf{y} = \mathbf{b}$$
--

- This expression is called **the normal equations** and it would be nice for 
you to memorize.

---

## Illustration with simple regression

.smallish[
.pull-left[
```{r, echo = TRUE}
data(anscombe)
# the "design matrix"; add 1s for the intercept
X <- as.matrix(cbind(1,anscombe$x1))

# this is the outcome
y <- as.matrix(anscombe$y1)

# look at X
X

```
]

.pull-right[
```{r, echo = TRUE}
# look at y
y

```
]]

---

## Visualizing the data

```{r anscombe}

ggplot(anscombe, aes(y = y1, x = x1)) +
  geom_point() +
  labs(y = "Y",
       x = "X",
       title = "Anscombe data (version 1)")

```


---

## Comparing `lm` and matrices

.pull-left[

```{r, echo = TRUE}
lm(y1 ~ x1 , data = anscombe) %>% 
  broom::tidy() %>% 
  select(term, estimate)
```
]

--

.pull-right[
```{r, echo = TRUE}
b <- solve(t(X) %*% X ) %*% t(X) %*% y
b

```

]

--

.center[**They are the same!**]

---

## The regression line

```{r anscombe_ols}

ggplot(anscombe, aes(y = y1, x = x1)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(y = "Y",
       x = "X",
       title = "Anscombe data (version 1)")

```

---

## Getting the rest of the information

.pull-left[

- If we want to get $\mathbf{e}$, we can. It will end up being useful.

```{r, echo = TRUE}
e <- y - X %*% b

```

- It turns out that $\mathbf{e^Te}$ is the sum of squared deviations of the
residuals.

```{r, echo = TRUE}
ssd <- t(e) %*% e

```

]

--

.pull-right[

- The error variance, $\sigma^2$, is just this value divided by $n-k$, where $n$
is the number of observations and $k$ is the number of columns in $\mathbf{X}$. 
So $\sigma^2=\frac{\mathbf{e^Te}}{n-k}$.

```{r, echo = TRUE}
sigma2 <- ssd / (length(y) - ncol(X))

# want to store this as a scalar
sigma2 <- as.numeric(sigma2)
sigma2 

```

]

---

## Getting standard errors

.smallish[
.pull-left[

- With the error variance, $\sigma ^2$, we can calculate the standard errors of
$\hat{\beta_0}$ and $\hat{\beta_1}$.

- To get the SEs, we first need the **variance-covariance** matrix of the 
estimates, which is just putting two things together we've already calculated:
$\sigma^2\mathbf{(X^TX)^{-1}}$.

- The SEs of the $\beta$s are the square roots of the diagonal elements of this 
matrix.

]]

--

.smallish[
.pull-right[

```{r, echo = TRUE}
invXtX <- solve( t(X) %*% X )
sigma2 * invXtX

sqrt(diag( sigma2 * invXtX ))

lm(y1 ~ x1, data = anscombe) %>% 
  broom::tidy() %>% 
  select(term, std.error)


```
]]

---

## Why am I teaching you this?

- If we were only doing simple regression, this would be overkill. But the 
nice part about the matrix representation is that the same exact steps work
just as well with (say) 100,000 cases and 25 predictor variables. This makes
things easy for **multiple regression.**

--

- The only "trick" is getting the matrix inverse, which is extremely fast for 
computers these days. Everything else is just arithmetic and something a lot
like good-old-fashioned middle-school algebra.

---

class:center, middle, inverse

# Diagnosing problems

---

## Diagnostics are the "check engine" of OLS

- Leverage

- Standardized residuals

- Cook's distance

---

## Leverage

- **Leverage** is how much "say" each data point gets when determining the $\beta$s.

- The farther away from the mean of (each) $x$, the more leverage a point has.

---

## Visualizing leverage

```{r leverage}
ggplot(anscombe, aes(y = y1, x = x1)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(y = "Y",
       x = "X",
       title = "Anscombe data (version 1)")

```

---

## Calculating leverage

- We calculate leverage using the **hat matrix**, $\mathbf{H}$.

- $\mathbf{H}$ is called the "hat" matrix because it changes $\mathbf{y}$ (the vector of responses) into $\mathbf{\hat{y}}$ (a vector of predicted values).
--

$$\mathbf{\hat{y}} = \mathbf{H}\mathbf{y} $$
--

$$\mathbf{H} = \mathbf{X}\mathbf{(X^TX)^{-1}}\mathbf{X^T}$$
--

$\mathbf{H}$ is a $n \times n$ square matrix. Its diagonal elements, $h_{ii}$, are the 
leverage. As you can see from the formula, nothing about the outcome is used
to calculate it.

---

## Seeing leverage

```{r}
H <- X %*% invXtX %*% t(X) 

ggplot(anscombe, aes(y = y1, x = x1,
                     label = round(diag(H), 3)  )) +
  geom_text() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(y = "Y",
       x = "X",
       title = "Anscombe data (version 1)",
       subtitle = "with each point's leverage")
```

---

## How much is too much?

- Leverage always adds up to the number of columns in $\mathbf{X}$ (so 2 here).

--

- A rule of thumb is that we get concerned when a point's leverage is greater 
than $\frac{2k}{n}$.

--

- Here that's $\frac{2(2)}{11} \approx .364$. So we have no particularly 
worrisome points.

--

- There's no rule about what to do with high leverage points. They just merit
closer attention because they can have a large effect on the $\hat{\beta}$s.

---

## Standardized residuals

- Since leverage uses only information about $\mathbf{X}$, it doesn't tell
us how well the model fits each point.

--

- We compute the standardized residual as $\frac{e_i}{\sigma \sqrt{1-h_{ii}}}$.

--

- This is a z-score that tells us roughly how "surprising" that point would be
if it were generated by the estimated regression model. A rule of thumb here
is that values of 3 or more warrant close inspection.

---

## Cook's distance

- This is a combination of leverage and residual that tells us how much the 
regression line would change if we omitted this particular observation.

- It measures the distance between all the $\hat{y_i}$ values for models estimated
with and without that particular observation (scaled by the error variance).

--

- There are several upper-bound rules of thumb for Cook's distance. The
most common are $4/n$, .5, and 1.

---

## Getting these in R

The easiest way to get these diagnostic values is to use `broom::augment`.

.smallish[
```{r, echo = TRUE}
fit <- lm(y1 ~ x1, data = anscombe)
d <- broom::augment(fit)
d

```
]

---

## Quick visualization

```{r infplot }
plot(fit, which = 5)

```

---

## Another version of Anscombe

```{r anscombe3 }
ggplot(anscombe, aes(y = y3, x = x3)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(y = "Y",
       x = "X",
       title = "Anscombe data (version 3)")
```

---

## Finding an influential case

.smallish[
```{r infplot2, echo = TRUE, out.width="70%"}
plot(lm(y3 ~ x3, data = anscombe), which = 5)

```
]


